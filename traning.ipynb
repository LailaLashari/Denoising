{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import (\n",
    "    Callback,\n",
    "    LearningRateScheduler,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "from tensorflow.keras.layers import (\n",
    "    Layer,\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    UpSampling2D,\n",
    "    Concatenate,\n",
    "    Add,\n",
    "    Activation,\n",
    "    GlobalAveragePooling2D,\n",
    "    GlobalMaxPooling2D,\n",
    "    Dense,\n",
    "    Reshape,\n",
    "    Lambda,\n",
    "    BatchNormalization,\n",
    "    Multiply\n",
    ")\n",
    "from tensorflow.keras import backend as K\n",
    "import glob\n",
    "import zipfile\n",
    "\n",
    "# Decorate custom functions for serialization\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def mean_axis_last(x):\n",
    "    return K.mean(x, axis=-1, keepdims=True)\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def max_axis_last(x):\n",
    "    return K.max(x, axis=-1, keepdims=True)\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def batch_dot_axes(x):\n",
    "    return K.batch_dot(x[0], x[1], axes=[2, 2])\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def batch_dot(x):\n",
    "    return K.batch_dot(x[0], x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import scipy.io\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Add, Activation,\n",
    "                                     Multiply, GlobalAveragePooling2D, Reshape, Dense, Lambda)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ModelCheckpoint, LearningRateScheduler, Callback)\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import scipy.io\n",
    "\n",
    "# Function to load images from directories\n",
    "def load_images(noisy_dir, clean_dir, image_size=(128, 128)):\n",
    "    noisy_images = []\n",
    "    clean_images = []\n",
    "\n",
    "    # Sort files to ensure alignment\n",
    "    noisy_files = sorted(os.listdir(noisy_dir))\n",
    "    clean_files = sorted(os.listdir(clean_dir))\n",
    "\n",
    "    for noisy_file, clean_file in zip(noisy_files, clean_files):\n",
    "        noisy_path = os.path.join(noisy_dir, noisy_file)\n",
    "        clean_path = os.path.join(clean_dir, clean_file)\n",
    "\n",
    "        # Read images\n",
    "        noisy_image = cv2.imread(noisy_path)\n",
    "        clean_image = cv2.imread(clean_path)\n",
    "\n",
    "        if noisy_image is None or clean_image is None:\n",
    "            print(f\"Warning: Skipping unmatched or unreadable file pair: {noisy_file}, {clean_file}\")\n",
    "            continue\n",
    "\n",
    "        # Resize images to target size\n",
    "        noisy_image = cv2.resize(noisy_image, image_size)\n",
    "        clean_image = cv2.resize(clean_image, image_size)\n",
    "\n",
    "        # Normalize images to [0, 1]\n",
    "        noisy_images.append(noisy_image / 255.0)\n",
    "        clean_images.append(clean_image / 255.0)\n",
    "\n",
    "    return np.array(noisy_images), np.array(clean_images)\n",
    "\n",
    "# Function to load test data from .mat files\n",
    "def load_data_from_mat(noisy_path, clean_path):\n",
    "    noisy_data = scipy.io.loadmat(noisy_path)\n",
    "    clean_data = scipy.io.loadmat(clean_path)\n",
    "\n",
    "    # Extract data from .mat structure\n",
    "    noisy_images = noisy_data.get(\"ValidationNoisyBlocksSrgb\", None)\n",
    "    clean_images = clean_data.get(\"ValidationGtBlocksSrgb\", None)\n",
    "\n",
    "    if noisy_images is None or clean_images is None:\n",
    "        raise ValueError(\"Invalid keys in .mat files. Ensure keys are 'ValidationNoisyBlocksSrgb' and 'ValidationGtBlocksSrgb'.\")\n",
    "\n",
    "    # Reshape and normalize data\n",
    "    noisy_images = noisy_images / 255.0\n",
    "    clean_images = clean_images / 255.0\n",
    "\n",
    "    # Flatten the blocks into a list of images\n",
    "    noisy_images = noisy_images.reshape((-1, *noisy_images.shape[2:]))\n",
    "    clean_images = clean_images.reshape((-1, *clean_images.shape[2:]))\n",
    "\n",
    "    return noisy_images, clean_images\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Paths\n",
    "train_noisy_dir = \" /Final_DN_Traning/DSENet/dataset/SSID_new/train/SIDD/input_crops\"\n",
    "train_clean_dir = \" /Final_DN_Traning/DSENet/dataset/SSID_new/train/SIDD/target_crops\"\n",
    "val_noisy_dir = \" /Final_DN_Traning/DSENet/dataset/SSID_new/val/SIDD/input_crops\"\n",
    "val_clean_dir = \" /Final_DN_Traning/DSENet/dataset/SSID_new/val/SIDD/target_crops\"\n",
    "test_noisy_path = \" /Final_DN_Traning/DSENet/dataset/SSID_new/test/SIDD/ValidationNoisyBlocksSrgb.mat\"\n",
    "test_clean_path = \" /Final_DN_Traning/DSENet/dataset/SSID_new/test/SIDD/ValidationGtBlocksSrgb.mat\"\n",
    "\n",
    "# Load training and validation datasets\n",
    "print(\"Loading training data...\")\n",
    "X_train, y_train = load_images(train_noisy_dir, train_clean_dir)\n",
    "\n",
    "print(\"Loading validation data...\")\n",
    "X_val, y_val = load_images(val_noisy_dir, val_clean_dir)\n",
    "\n",
    "# Print dataset shapes\n",
    "print(f\"Training Data: Noisy {X_train.shape}, Clean {y_train.shape}\")\n",
    "print(f\"Validation Data: Noisy {X_val.shape}, Clean {y_val.shape}\")\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "# Load test data from .mat files\n",
    "X_test, y_test = load_data_from_mat(test_noisy_path, test_clean_path)\n",
    "\n",
    "# Print dataset shapes for verification\n",
    "print(f\"Training Data (Original): Noisy {X_train.shape}, Clean {y_train.shape}\")\n",
    "print(f\"Validation Data: Noisy {X_val.shape}, Clean {y_val.shape}\")\n",
    "print(f\"Test Data: Noisy {X_test.shape}, Clean {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# Run your model again\n",
    "# import tensorflow as tf\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)\n",
    "\n",
    "\n",
    "# TQDM Progress Bar Callback\n",
    "class TQDMProgressBar(Callback):\n",
    "    def __init__(self, total_steps, update_interval=0.1):\n",
    "        super().__init__()\n",
    "        self.total_steps = total_steps\n",
    "        self.update_interval = update_interval\n",
    "        self.steps_per_update = max(1, int(self.total_steps * self.update_interval))\n",
    "        self.total_epochs = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.total_epochs = self.params['epochs']\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = epoch\n",
    "        self.tqdm = tqdm(total=self.total_steps, desc=f'Epoch {epoch+1}/{self.total_epochs}', position=0, leave=True)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.tqdm.update(1)\n",
    "        if batch % self.steps_per_update == 0:\n",
    "            loss = logs.get('loss')\n",
    "            psnr = logs.get('psnr_metric')\n",
    "            ssim = logs.get('ssim_metric')\n",
    "            self.tqdm.set_postfix(loss=loss, PSNR=psnr, SSIM=ssim)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.tqdm.close()\n",
    "        epoch_time = time.time() - self.start_time\n",
    "        loss = logs.get('loss', 0)\n",
    "        psnr = logs.get('psnr_metric', 0)\n",
    "        ssim = logs.get('ssim_metric', 0)\n",
    "        print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} seconds - loss: {loss:.4f}, PSNR: {psnr:.4f}, SSIM: {ssim:.4f}\")\n",
    "\n",
    "# Custom Callback for logging\n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Average loss in this epoch is: {logs['loss']:.4f}\")\n",
    "        print(f\"PSNR: {logs['psnr_metric']:.4f}, SSIM: {logs['ssim_metric']:.4f}\")\n",
    "\n",
    "# Function to load images\n",
    "def load_images(noisy_dir, clean_dir, image_size=(128, 128), max_images=20000):\n",
    "    noisy_images = []\n",
    "    clean_images = []\n",
    "    noisy_pattern = os.path.join(noisy_dir, '*.*')\n",
    "    clean_pattern = os.path.join(clean_dir, '*.*')\n",
    "    noisy_files = sorted(glob.glob(noisy_pattern))[:max_images]\n",
    "    clean_files = sorted(glob.glob(clean_pattern))[:max_images]\n",
    "\n",
    "    for noisy_path, clean_path in zip(noisy_files, clean_files):\n",
    "        noisy_image = cv2.imread(noisy_path)\n",
    "        if noisy_image is None:\n",
    "            print(f\"Warning: Unable to read noisy image file {noisy_path}\")\n",
    "            continue\n",
    "\n",
    "        clean_image = cv2.imread(clean_path)\n",
    "        if clean_image is None:\n",
    "            print(f\"Warning: Unable to read clean image file {clean_path}\")\n",
    "            continue\n",
    "\n",
    "        noisy_image = cv2.resize(noisy_image, image_size)\n",
    "        clean_image = cv2.resize(clean_image, image_size)\n",
    "\n",
    "        noisy_images.append(noisy_image / 255.0)\n",
    "        clean_images.append(clean_image / 255.0)\n",
    "\n",
    "    return np.array(noisy_images), np.array(clean_images)\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, GlobalAveragePooling2D, Reshape, Dense, Multiply,\n",
    "    Conv2D, Add, Activation, Lambda, MaxPooling2D, UpSampling2D,\n",
    "    Concatenate, LayerNormalization, Dropout\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import lecun_normal\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Squeeze-and-Excitation Block\n",
    "def se_block(input_tensor, ratio=16):\n",
    "    channel_axis = -1\n",
    "    filters = input_tensor.shape[channel_axis]\n",
    "    se_shape = (1, 1, filters)\n",
    "\n",
    "    se = GlobalAveragePooling2D()(input_tensor)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    se = Dense(max(1, filters // ratio), activation='relu', kernel_initializer=lecun_normal(), use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer=lecun_normal(), use_bias=False)(se)\n",
    "    x = Multiply()([input_tensor, se])\n",
    "    return x\n",
    "\n",
    "# Attention-SE Block Hybrid\n",
    "def attention_se_block(x, g, inter_channel):\n",
    "    x = se_block(x)\n",
    "    \n",
    "    theta_x = Conv2D(inter_channel, 1, padding='same', kernel_initializer=lecun_normal())(x)\n",
    "    phi_g = Conv2D(inter_channel, 1, padding='same', kernel_initializer=lecun_normal())(g)\n",
    "    add_xg = Add()([theta_x, phi_g])\n",
    "    selu_xg = Activation('relu')(add_xg)\n",
    "    psi = Conv2D(1, 1, padding='same', kernel_initializer=lecun_normal())(selu_xg)\n",
    "    sigmoid_xg = Activation('sigmoid')(psi)\n",
    "    upsample_psi = Multiply()([sigmoid_xg, x])\n",
    "    return upsample_psi\n",
    "\n",
    "# Self-Attention-SE Block\n",
    "def self_attention_se_block(inputs):\n",
    "    inputs = se_block(inputs)\n",
    "\n",
    "    channels = K.int_shape(inputs)[-1]\n",
    "    reduced_channels = max(1, channels // 8)\n",
    "\n",
    "    f = Conv2D(reduced_channels, kernel_size=1, padding='same', kernel_initializer=lecun_normal())(inputs)  # key\n",
    "    g = Conv2D(reduced_channels, kernel_size=1, padding='same', kernel_initializer=lecun_normal())(inputs)  # query\n",
    "    h = Conv2D(channels, kernel_size=1, padding='same', kernel_initializer=lecun_normal())(inputs)          # value\n",
    "\n",
    "    f_flatten = Reshape((-1, reduced_channels))(f)\n",
    "    g_flatten = Reshape((-1, reduced_channels))(g)\n",
    "    h_flatten = Reshape((-1, channels))(h)\n",
    "\n",
    "    s = Lambda(lambda x: tf.matmul(x[0], x[1], transpose_b=True))([g_flatten, f_flatten])\n",
    "    beta = Activation('softmax')(s)\n",
    "\n",
    "    o = Lambda(lambda x: tf.matmul(x[0], x[1]))([beta, h_flatten])\n",
    "\n",
    "    o_reshaped = Reshape(K.int_shape(inputs)[1:])(o)\n",
    "\n",
    "    x = Add()([o_reshaped, inputs])\n",
    "\n",
    "    return x\n",
    "\n",
    "# Multi-scale block (simplified)\n",
    "def multi_scale_block(inputs, filters):\n",
    "    conv3x3 = Conv2D(filters, kernel_size=3, padding='same', activation='relu', kernel_initializer=lecun_normal())(inputs)\n",
    "    conv5x5 = Conv2D(filters, kernel_size=5, padding='same', activation='relu', kernel_initializer=lecun_normal())(inputs)\n",
    "\n",
    "    x = Concatenate()([conv3x3, conv5x5])\n",
    "    return x\n",
    "\n",
    "# Updated conv_block with SELU\n",
    "def conv_block(inputs, filters, kernel_size=3, padding='same'):\n",
    "    x = multi_scale_block(inputs, filters)\n",
    "    shortcut = inputs\n",
    "\n",
    "    if inputs.shape[-1] != filters * 2:\n",
    "        shortcut = Conv2D(filters * 2, kernel_size=1, padding='same', kernel_initializer=lecun_normal())(inputs)\n",
    "\n",
    "    x = Conv2D(filters * 2, kernel_size, padding=padding, kernel_initializer=lecun_normal())(x)\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)  # Activation with SELU\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute filters for PyramidNet with linear increment\n",
    "def compute_pyramid_filters(num_blocks, initial_filters, increment):\n",
    "    filters_list = [initial_filters + i * increment for i in range(num_blocks)]\n",
    "    return filters_list\n",
    "\n",
    "class SwinTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, num_heads, window_size=7, mlp_ratio=4., dropout=0.0, **kwargs):\n",
    "        super(SwinTransformerBlock, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Simplified attention mechanism\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n",
    "        self.projection = Dense(dim)  # Align attention output back to `dim`\n",
    "        \n",
    "        # Reduced MLP layer\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp_dense = Dense(int(dim * mlp_ratio), activation='relu')  # Reduced to single layer MLP\n",
    "        self.mlp_output_projection = Dense(dim)  # Project back to `dim`\n",
    "        \n",
    "        self.dropout_layer = Dropout(rate=dropout)\n",
    "        \n",
    "\n",
    "def decoder_block(inputs, skip_features, filters, num_heads=8, window_size=7):\n",
    "    # Step 1: Upsampling the inputs\n",
    "    x = UpSampling2D(size=(2, 2))(inputs)\n",
    "    \n",
    "    # Step 2: Apply Attention-SE Block on the skip features\n",
    "    skip_features = attention_se_block(skip_features, x, filters)\n",
    "    \n",
    "    # Step 3: Concatenate upsampled features with skip features\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    \n",
    "    # Step 4: Apply Convolutional Block to refine combined features\n",
    "    x = conv_block(x, filters)\n",
    "    \n",
    "    # Step 5: Apply Swin Transformer Block for further feature refinement\n",
    "    x = SwinTransformerBlock(dim=filters, num_heads=num_heads, window_size=window_size)(x)\n",
    "    \n",
    "    return x\n",
    "    \n",
    "# Encoder Block with Swin Transformer Integration\n",
    "def encoder_block_with_swin(inputs, filters, num_heads=8, window_size=7):\n",
    "    # Convolutional block\n",
    "    x = conv_block(inputs, filters)\n",
    "    x = se_block(x)  # Add SE block\n",
    "\n",
    "    # Add Swin Transformer block\n",
    "    x = SwinTransformerBlock(dim=filters, num_heads=num_heads, window_size=window_size)(x)\n",
    "    \n",
    "    # Pooling\n",
    "    p = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "# Updated U-Net with Swin Transformer\n",
    "def unet_pyramid_model(input_shape=(128, 128, 3)):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    num_encoder_blocks = 4  # Number of encoder blocks\n",
    "    initial_filters = 32    # Starting number of filters\n",
    "    increment = 32          # Linear increment of filters per block\n",
    "    \n",
    "    # Compute filters for encoder and bottleneck\n",
    "    encoder_filters = compute_pyramid_filters(num_encoder_blocks + 1, initial_filters, increment)\n",
    "    \n",
    "    # Encoder\n",
    "    s1, p1 = encoder_block_with_swin(inputs, encoder_filters[0])  # 32 filters\n",
    "    s2, p2 = encoder_block_with_swin(p1, encoder_filters[1])      # 64 filters\n",
    "    s3, p3 = encoder_block_with_swin(p2, encoder_filters[2])      # 96 filters\n",
    "    s4, p4 = encoder_block_with_swin(p3, encoder_filters[3])      # 128 filters\n",
    "    \n",
    "    # Bottleneck\n",
    "    b1 = conv_block(p4, encoder_filters[4])                       # 160 filters\n",
    "    b1 = self_attention_se_block(b1)\n",
    "    \n",
    "    # Compute filters for decoder (reverse of encoder filters)\n",
    "    decoder_filters = encoder_filters[:-1][::-1]                  # [128, 96, 64, 32]\n",
    "    \n",
    "    # Decoder\n",
    "    d1 = decoder_block(b1, s4, decoder_filters[0])                # 128 filters\n",
    "    d2 = decoder_block(d1, s3, decoder_filters[1])                # 96 filters\n",
    "    d3 = decoder_block(d2, s2, decoder_filters[2])                # 64 filters\n",
    "    d4 = decoder_block(d3, s1, decoder_filters[3])                # 32 filters\n",
    "    \n",
    "    # Output\n",
    "    outputs = Conv2D(3, 1, padding='same', activation='sigmoid')(d4)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 10:\n",
    "        lr = lr * tf.math.exp(-0.1)\n",
    "    return float(lr)\n",
    "\n",
    "# Define PSNR Metric\n",
    "def psnr_metric(y_true, y_pred):\n",
    "    return tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
    "\n",
    "# Define SSIM Metric\n",
    "def ssim_metric(y_true, y_pred):\n",
    "    return tf.image.ssim(y_true, y_pred, max_val=1.0)\n",
    "\n",
    "def compute_metrics(original_images, denoised_images):\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "\n",
    "    for i in range(len(original_images)):\n",
    "        psnr = peak_signal_noise_ratio(original_images[i], denoised_images[i], data_range=1.0)\n",
    "        ssim = structural_similarity(original_images[i], denoised_images[i], channel_axis=-1, data_range=1.0, win_size=3)\n",
    "\n",
    "        psnr_values.append(psnr)\n",
    "        ssim_values.append(ssim)\n",
    "\n",
    "    return psnr_values, ssim_values\n",
    "\n",
    "# Learning Rate Logger Callback\n",
    "class LearningRateLogger(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        # Get the optimizer from the model\n",
    "        optimizer = self.model.optimizer\n",
    "        # Access the learning rate\n",
    "        lr = optimizer.learning_rate\n",
    "\n",
    "        # If the learning rate is a schedule, get the current value\n",
    "        if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            # Evaluate the schedule at the current iteration\n",
    "            lr = lr(self.model.optimizer.iterations)\n",
    "        else:\n",
    "            # If it's a constant, get the value directly\n",
    "            lr = lr\n",
    "\n",
    "        # Convert the learning rate to a float value\n",
    "        lr = K.get_value(lr)\n",
    "        # Log the learning rate\n",
    "        logs['lr'] = lr\n",
    "        print(f\"Current learning rate: {lr}\")\n",
    "\n",
    "# Instantiate the learning rate logger\n",
    "lr_logger = LearningRateLogger()\n",
    "\n",
    "# Function to plot training history\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], 'ro-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot learning rate\n",
    "    if 'lr' in history.history:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, history.history['lr'], 'go-', label='Learning Rate')\n",
    "        plt.title('Learning Rate over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot PSNR and SSIM metrics\n",
    "def plot_psnr_ssim(psnr_values, ssim_values):\n",
    "    indices = range(len(psnr_values))\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot PSNR values\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(indices, psnr_values, 'bo-', label='PSNR')\n",
    "    plt.title('PSNR Values on Test Set')\n",
    "    plt.xlabel('Image Index')\n",
    "    plt.ylabel('PSNR')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot SSIM values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(indices, ssim_values, 'ro-', label='SSIM')\n",
    "    plt.title('SSIM Values on Test Set')\n",
    "    plt.xlabel('Image Index')\n",
    "    plt.ylabel('SSIM')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Custom Loss Functions converted from PyTorch to TensorFlow\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = tf.exp(-tf.square(tf.range(window_size, dtype=tf.float32) - window_size // 2) / (2 * sigma**2))\n",
    "    return gauss / tf.reduce_sum(gauss)\n",
    "\n",
    "def get_gaussian_kernel(ksize, sigma):\n",
    "    if not isinstance(ksize, int) or ksize % 2 == 0 or ksize <= 0:\n",
    "        raise TypeError(f\"ksize must be an odd positive integer. Got {ksize}\")\n",
    "    return gaussian(ksize, sigma)\n",
    "\n",
    "def get_gaussian_kernel2d(ksize, sigma):\n",
    "    if not isinstance(ksize, tuple) or len(ksize) != 2:\n",
    "        raise TypeError(f\"ksize must be a tuple of length two. Got {ksize}\")\n",
    "    if not isinstance(sigma, tuple) or len(sigma) != 2:\n",
    "        raise TypeError(f\"sigma must be a tuple of length two. Got {sigma}\")\n",
    "    \n",
    "    kernel_x = get_gaussian_kernel(ksize[0], sigma[0])\n",
    "    kernel_y = get_gaussian_kernel(ksize[1], sigma[1])\n",
    "    \n",
    "    kernel_2d = tf.tensordot(kernel_x, kernel_y, axes=0)\n",
    "    return kernel_2d\n",
    "\n",
    "# PSNR Loss\n",
    "class PSNRLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, loss_weight=1.0, toY=False):\n",
    "        super(PSNRLoss, self).__init__()\n",
    "        self.loss_weight = loss_weight\n",
    "        self.scale = 10 / tf.math.log(10.0)\n",
    "        self.toY = toY\n",
    "        self.coef = tf.constant([65.481, 128.553, 24.966], shape=(1, 1, 1, 3))\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        if self.toY:\n",
    "            y_true = tf.reduce_sum(y_true * self.coef, axis=-1, keepdims=True) + 16\n",
    "            y_pred = tf.reduce_sum(y_pred * self.coef, axis=-1, keepdims=True) + 16\n",
    "            \n",
    "            y_true = y_true / 255.0\n",
    "            y_pred = y_pred / 255.0\n",
    "        \n",
    "        mse_loss = tf.reduce_mean(tf.square(y_pred - y_true), axis=[1, 2, 3])\n",
    "        loss = -self.loss_weight * self.scale * tf.reduce_mean(tf.math.log(mse_loss + 1e-8))\n",
    "        return loss\n",
    "\n",
    "# SSIM Loss\n",
    "class SSIMLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, window_size=11, max_val=1.0):\n",
    "        super(SSIMLoss, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.max_val = max_val\n",
    "        self.window = get_gaussian_kernel2d((window_size, window_size), (1.5, 1.5))\n",
    "        self.padding = (window_size - 1) // 2\n",
    "\n",
    "        self.C1 = (0.01 * self.max_val) ** 2\n",
    "        self.C2 = (0.03 * self.max_val) ** 2\n",
    "    \n",
    "    def filter2D(self, img, kernel):\n",
    "        kernel = kernel[:, :, tf.newaxis, tf.newaxis]\n",
    "        kernel = tf.tile(kernel, [1, 1, img.shape[-1], 1])\n",
    "        return tf.nn.depthwise_conv2d(img, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        kernel = self.window\n",
    "        mu1 = self.filter2D(y_true, kernel)\n",
    "        mu2 = self.filter2D(y_pred, kernel)\n",
    "\n",
    "        mu1_sq = tf.square(mu1)\n",
    "        mu2_sq = tf.square(mu2)\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "\n",
    "        sigma1_sq = self.filter2D(y_true * y_true, kernel) - mu1_sq\n",
    "        sigma2_sq = self.filter2D(y_pred * y_pred, kernel) - mu2_sq\n",
    "        sigma12 = self.filter2D(y_true * y_pred, kernel) - mu1_mu2\n",
    "\n",
    "        ssim_map = ((2 * mu1_mu2 + self.C1) * (2 * sigma12 + self.C2)) / \\\n",
    "                   ((mu1_sq + mu2_sq + self.C1) * (sigma1_sq + sigma2_sq + self.C2))\n",
    "\n",
    "        return tf.reduce_mean((1.0 - ssim_map) / 2.0)\n",
    "\n",
    "# Charbonnier Loss\n",
    "class CharbonnierLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, eps=1e-3):\n",
    "        super(CharbonnierLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        diff = y_pred - y_true\n",
    "        loss = tf.reduce_mean(tf.sqrt(tf.square(diff) + self.eps**2))\n",
    "        return loss\n",
    "\n",
    "# Edge Loss\n",
    "class EdgeLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(EdgeLoss, self).__init__()\n",
    "        k = tf.constant([[0.05, 0.25, 0.4, 0.25, 0.05]], dtype=tf.float32)\n",
    "        self.kernel = tf.matmul(k, k, transpose_b=True)\n",
    "        self.kernel = self.kernel[:, :, tf.newaxis, tf.newaxis]\n",
    "        self.loss_fn = CharbonnierLoss()\n",
    "\n",
    "    def conv_gauss(self, img):\n",
    "        n_channels = img.shape[-1]\n",
    "        kernel = tf.tile(self.kernel, [1, 1, n_channels, 1])\n",
    "        return tf.nn.depthwise_conv2d(img, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    def laplacian_kernel(self, img):\n",
    "        filtered = self.conv_gauss(img)\n",
    "        downsampled = filtered[:, ::2, ::2, :]\n",
    "        upsampled = tf.image.resize(downsampled, filtered.shape[1:3])\n",
    "        filtered_up = self.conv_gauss(upsampled)\n",
    "        return img - filtered_up\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        return self.loss_fn(self.laplacian_kernel(y_true), self.laplacian_kernel(y_pred))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Load images\n",
    "\n",
    "noisy_dir = '/Final_DN_Traning/DSENet/dataset/complete_merged_dataset/train/input/'\n",
    "clean_dir = '/Final_DN_Traning/DSENet/dataset/complete_merged_dataset/train/groundtruth'\n",
    "X, y = load_images(noisy_dir, clean_dir)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.1, random_state=42)\n",
    "# Split data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# total_steps = len(X_train) // batch_size\n",
    "# progress_bar_callback = TQDMProgressBar(total_steps=total_steps)\n",
    "\n",
    "batch_size = 16\n",
    "total_epochs = 100\n",
    "\n",
    "# Create U-Net model\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "# Adjust early stopping to monitor PSNR instead of loss\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_psnr_metric',  # Change to 'val_ssim_metric' if SSIM is preferred\n",
    "    patience=5,\n",
    "    mode='max',  # PSNR and SSIM are maximization metrics\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Model Checkpoint for the entire model\n",
    "best_model_checkpoint = ModelCheckpoint(\n",
    "    filepath='Checkpoint/HSENet.keras',\n",
    "    monitor='val_psnr_metric',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Model Checkpoint for only the weights\n",
    "best_weights_checkpoint = ModelCheckpoint(\n",
    "    filepath='Checkpoint/HSENet.weights.h5',\n",
    "    monitor='val_psnr_metric',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Learning rate scheduler callback\n",
    "scheduler_callback = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "total_steps = len(X_train) // batch_size\n",
    "progress_bar_callback = TQDMProgressBar(total_steps=total_steps)\n",
    "\n",
    "# Compile model\n",
    "model = unet_pyramid_model(input_shape)\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "\n",
    "# Define combined loss function\n",
    "def combined_loss(y_true, y_pred):\n",
    "    charbonnier = CharbonnierLoss()(y_true, y_pred)\n",
    "    edge = EdgeLoss()(y_true, y_pred)\n",
    "    ssim = SSIMLoss()(y_true, y_pred)\n",
    "    return charbonnier + edge + ssim\n",
    "\n",
    "# Compile the model with the combined loss function\n",
    "model.compile(optimizer=optimizer, loss=combined_loss, metrics=[psnr_metric, ssim_metric])\n",
    "model.summary()\n",
    "\n",
    "# Start timing for training\n",
    "training_start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=total_epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        CustomCallback(),\n",
    "        scheduler_callback,\n",
    "        early_stopping_callback,\n",
    "        best_model_checkpoint,\n",
    "        best_weights_checkpoint,\n",
    "        progress_bar_callback, \n",
    "        lr_logger\n",
    "    ],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# End timing for training\n",
    "training_time = time.time() - training_start_time\n",
    "print(f\"Total training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)\n",
    "\n",
    "# Evaluate on validation set\n",
    "denoised_images_val = model.predict(X_val)\n",
    "psnr_values_val, ssim_values_val = compute_metrics(y_val, denoised_images_val)\n",
    "mean_psnr_val = np.mean(psnr_values_val)\n",
    "mean_ssim_val = np.mean(ssim_values_val)\n",
    "print(f\"Validation PSNR: {mean_psnr_val:.4f}, Validation SSIM: {mean_ssim_val:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "denoised_images_test = model.predict(X_test)\n",
    "psnr_values_test, ssim_values_test = compute_metrics(y_test, denoised_images_test)\n",
    "mean_psnr_test = np.mean(psnr_values_test)\n",
    "mean_ssim_test = np.mean(ssim_values_test)\n",
    "\n",
    "print(f\"\\nTest PSNR: {mean_psnr_test:.4f}\")\n",
    "print(f\"Test SSIM: {mean_ssim_test:.4f}\")\n",
    "\n",
    "# Plot PSNR and SSIM metrics\n",
    "plot_psnr_ssim(psnr_values_test, ssim_values_test)\n",
    "\n",
    "# Save and zip the model\n",
    "model_save_path = '/Final_DN_Traning/DSENet/DN_model/HSENet.keras'\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "zip_save_path = '/Final_DN_Traning/DSENet/DN_model/HSENet.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_save_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(model_save_path, arcname=os.path.basename(model_save_path))\n",
    "print(f\"Model zipped and saved to {zip_save_path}\")\n",
    "\n",
    "# Save denoised images\n",
    "denoised_image_save_path = '/Final_DN_Traning/DSENet/DN_model/HSENet'\n",
    "os.makedirs(denoised_image_save_path, exist_ok=True)\n",
    "for i, denoised_image in enumerate(denoised_images_test):\n",
    "    denoised_image_path = os.path.join(denoised_image_save_path, f'{i}.png')\n",
    "    cv2.imwrite(denoised_image_path, (denoised_image * 255).astype(np.uint8))\n",
    "\n",
    "# Visualize results\n",
    "def plot_results(noisy_images, clean_images, denoised_images, n=5):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(n):\n",
    "        plt.subplot(n, 3, i * 3 + 1)\n",
    "        plt.imshow(noisy_images[i])\n",
    "        plt.title('Noisy Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(n, 3, i * 3 + 2)\n",
    "        plt.imshow(clean_images[i])\n",
    "        plt.title('Clean Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(n, 3, i * 3 + 3)\n",
    "        plt.imshow(denoised_images[i])\n",
    "        plt.title('Denoised Image')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plot_results(X_test, y_test, denoised_images_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
