{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, GlobalAveragePooling2D, Reshape, Dense, Multiply,\n",
    "    Conv2D, Add, Activation, Lambda, MaxPooling2D, UpSampling2D,\n",
    "    Concatenate, LayerNormalization, Dropout\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import lecun_normal\n",
    "# Function to load images\n",
    "def load_images(noisy_dir, clean_dir, image_size=(128, 128), max_images=20000):\n",
    "    noisy_images = []\n",
    "    clean_images = []\n",
    "    noisy_pattern = os.path.join(noisy_dir, '*.*')\n",
    "    clean_pattern = os.path.join(clean_dir, '*.*')\n",
    "    noisy_files = sorted(glob.glob(noisy_pattern))[:max_images]\n",
    "    clean_files = sorted(glob.glob(clean_pattern))[:max_images]\n",
    "\n",
    "    for noisy_path, clean_path in zip(noisy_files, clean_files):\n",
    "        noisy_image = cv2.imread(noisy_path)\n",
    "        if noisy_image is None:\n",
    "            print(f\"Warning: Unable to read noisy image file {noisy_path}\")\n",
    "            continue\n",
    "\n",
    "        clean_image = cv2.imread(clean_path)\n",
    "        if clean_image is None:\n",
    "            print(f\"Warning: Unable to read clean image file {clean_path}\")\n",
    "            continue\n",
    "\n",
    "        noisy_image = cv2.resize(noisy_image, image_size)\n",
    "        clean_image = cv2.resize(clean_image, image_size)\n",
    "\n",
    "        noisy_images.append(noisy_image / 255.0)\n",
    "        clean_images.append(clean_image / 255.0)\n",
    "\n",
    "    return np.array(noisy_images), np.array(clean_images)\n",
    "\n",
    "\n",
    "# # Load the images\n",
    "\n",
    "# # Split the data into training, validation, and test sets\n",
    "noisy_dir = '/Final_DN_Traning/DSENet/dataset/complete_merged_dataset/train/input/'\n",
    "clean_dir = '/Final_DN_Traning/DSENet/dataset/complete_merged_dataset/train/groundtruth'\n",
    "X, y = load_images(noisy_dir, clean_dir)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test_full, y_train, y_test_full = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use only the first 400 images of the test set\n",
    "X_test, y_test = X_test_full[:4000], y_test_full[:4000]\n",
    "\n",
    "# Print summary of dataset splits\n",
    "print(f\"Total images: {len(X)}\")\n",
    "print(f\"Training set: {len(X_train)}\")\n",
    "print(f\"Test set (restricted): {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)\n",
    "\n",
    "\n",
    "# Define PSNR Metric\n",
    "def psnr_metric(y_true, y_pred):\n",
    "    return tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
    "\n",
    "# Define SSIM Metric\n",
    "def ssim_metric(y_true, y_pred):\n",
    "    return tf.image.ssim(y_true, y_pred, max_val=1.0)\n",
    "\n",
    "# Custom Loss Functions converted from PyTorch to TensorFlow\n",
    "# Combined loss function\n",
    "def combined_loss(y_true, y_pred):\n",
    "    charbonnier = CharbonnierLoss()(y_true, y_pred)\n",
    "    edge = EdgeLoss()(y_true, y_pred)\n",
    "    ssim = SSIMLoss()(y_true, y_pred)\n",
    "    return charbonnier + edge + ssim\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "# Custom Loss Functions converted from PyTorch to TensorFlow\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = tf.exp(-tf.square(tf.range(window_size, dtype=tf.float32) - window_size // 2) / (2 * sigma**2))\n",
    "    return gauss / tf.reduce_sum(gauss)\n",
    "\n",
    "def get_gaussian_kernel(ksize, sigma):\n",
    "    if not isinstance(ksize, int) or ksize % 2 == 0 or ksize <= 0:\n",
    "        raise TypeError(f\"ksize must be an odd positive integer. Got {ksize}\")\n",
    "    return gaussian(ksize, sigma)\n",
    "\n",
    "def get_gaussian_kernel2d(ksize, sigma):\n",
    "    if not isinstance(ksize, tuple) or len(ksize) != 2:\n",
    "        raise TypeError(f\"ksize must be a tuple of length two. Got {ksize}\")\n",
    "    if not isinstance(sigma, tuple) or len(sigma) != 2:\n",
    "        raise TypeError(f\"sigma must be a tuple of length two. Got {sigma}\")\n",
    "    \n",
    "    kernel_x = get_gaussian_kernel(ksize[0], sigma[0])\n",
    "    kernel_y = get_gaussian_kernel(ksize[1], sigma[1])\n",
    "    \n",
    "    kernel_2d = tf.tensordot(kernel_x, kernel_y, axes=0)\n",
    "    return kernel_2d\n",
    "# Define ENL Metric\n",
    "def enl_metric(image):\n",
    "    \"\"\"\n",
    "    Calculate the Enhanced Non-Local (ENL) metric.\n",
    "    ENL = (mean_intensity^2) / variance\n",
    "    \"\"\"\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    mean_intensity = tf.reduce_mean(image)\n",
    "    variance = tf.math.reduce_variance(image)\n",
    "    return tf.math.divide_no_nan(mean_intensity ** 2, variance)\n",
    "\n",
    "# Define FOM Metric\n",
    "def fom_metric(clean_image, denoised_image):\n",
    "    \"\"\"\n",
    "    Calculate the Figure of Merit (FOM).\n",
    "    FOM = SSIM(clean_image, denoised_image) * PSNR(clean_image, denoised_image)\n",
    "    \"\"\"\n",
    "    ssim_value = tf.image.ssim(clean_image, denoised_image, max_val=1.0)\n",
    "    psnr_value = tf.image.psnr(clean_image, denoised_image, max_val=1.0)\n",
    "    return ssim_value * psnr_value\n",
    "\n",
    "# PSNR Loss\n",
    "class PSNRLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, loss_weight=1.0, toY=False):\n",
    "        super(PSNRLoss, self).__init__()\n",
    "        self.loss_weight = loss_weight\n",
    "        self.scale = 10 / tf.math.log(10.0)\n",
    "        self.toY = toY\n",
    "        self.coef = tf.constant([65.481, 128.553, 24.966], shape=(1, 1, 1, 3))\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        if self.toY:\n",
    "            y_true = tf.reduce_sum(y_true * self.coef, axis=-1, keepdims=True) + 16\n",
    "            y_pred = tf.reduce_sum(y_pred * self.coef, axis=-1, keepdims=True) + 16\n",
    "            \n",
    "            y_true = y_true / 255.0\n",
    "            y_pred = y_pred / 255.0\n",
    "        \n",
    "        mse_loss = tf.reduce_mean(tf.square(y_pred - y_true), axis=[1, 2, 3])\n",
    "        loss = -self.loss_weight * self.scale * tf.reduce_mean(tf.math.log(mse_loss + 1e-8))\n",
    "        return loss\n",
    "\n",
    "# SSIM Loss\n",
    "class SSIMLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, window_size=11, max_val=1.0):\n",
    "        super(SSIMLoss, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.max_val = max_val\n",
    "        self.window = get_gaussian_kernel2d((window_size, window_size), (1.5, 1.5))\n",
    "        self.padding = (window_size - 1) // 2\n",
    "\n",
    "        self.C1 = (0.01 * self.max_val) ** 2\n",
    "        self.C2 = (0.03 * self.max_val) ** 2\n",
    "    \n",
    "    def filter2D(self, img, kernel):\n",
    "        kernel = kernel[:, :, tf.newaxis, tf.newaxis]\n",
    "        kernel = tf.tile(kernel, [1, 1, img.shape[-1], 1])\n",
    "        return tf.nn.depthwise_conv2d(img, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        kernel = self.window\n",
    "        mu1 = self.filter2D(y_true, kernel)\n",
    "        mu2 = self.filter2D(y_pred, kernel)\n",
    "\n",
    "        mu1_sq = tf.square(mu1)\n",
    "        mu2_sq = tf.square(mu2)\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "\n",
    "        sigma1_sq = self.filter2D(y_true * y_true, kernel) - mu1_sq\n",
    "        sigma2_sq = self.filter2D(y_pred * y_pred, kernel) - mu2_sq\n",
    "        sigma12 = self.filter2D(y_true * y_pred, kernel) - mu1_mu2\n",
    "\n",
    "        ssim_map = ((2 * mu1_mu2 + self.C1) * (2 * sigma12 + self.C2)) / \\\n",
    "                   ((mu1_sq + mu2_sq + self.C1) * (sigma1_sq + sigma2_sq + self.C2))\n",
    "\n",
    "        return tf.reduce_mean((1.0 - ssim_map) / 2.0)\n",
    "\n",
    "# Charbonnier Loss\n",
    "class CharbonnierLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, eps=1e-3):\n",
    "        super(CharbonnierLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        diff = y_pred - y_true\n",
    "        loss = tf.reduce_mean(tf.sqrt(tf.square(diff) + self.eps**2))\n",
    "        return loss\n",
    "\n",
    "# Edge Loss\n",
    "class EdgeLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(EdgeLoss, self).__init__()\n",
    "        k = tf.constant([[0.05, 0.25, 0.4, 0.25, 0.05]], dtype=tf.float32)\n",
    "        self.kernel = tf.matmul(k, k, transpose_b=True)\n",
    "        self.kernel = self.kernel[:, :, tf.newaxis, tf.newaxis]\n",
    "        self.loss_fn = CharbonnierLoss()\n",
    "\n",
    "    def conv_gauss(self, img):\n",
    "        n_channels = img.shape[-1]\n",
    "        kernel = tf.tile(self.kernel, [1, 1, n_channels, 1])\n",
    "        return tf.nn.depthwise_conv2d(img, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    def laplacian_kernel(self, img):\n",
    "        filtered = self.conv_gauss(img)\n",
    "        downsampled = filtered[:, ::2, ::2, :]\n",
    "        upsampled = tf.image.resize(downsampled, filtered.shape[1:3])\n",
    "        filtered_up = self.conv_gauss(upsampled)\n",
    "        return img - filtered_up\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        return self.loss_fn(self.laplacian_kernel(y_true), self.laplacian_kernel(y_pred))\n",
    "# Define or import all your custom classes and functions\n",
    "# Custom Layers\n",
    "class SwinTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, num_heads, window_size=7, mlp_ratio=4., dropout=0.0, **kwargs):\n",
    "        super(SwinTransformerBlock, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Simplified attention mechanism\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=dim)\n",
    "        self.projection = Dense(dim)  # Align attention output back to `dim`\n",
    "        \n",
    "        # Reduced MLP layer\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp_dense = Dense(int(dim * mlp_ratio), activation='relu')  # Reduced to single layer MLP\n",
    "        self.mlp_output_projection = Dense(dim)  # Project back to `dim`\n",
    "        \n",
    "        self.dropout_layer = Dropout(rate=dropout)\n",
    "        \n",
    "\n",
    "# Now, load the model using custom_objects\n",
    "model_save_path = '/Final_DN_Traning/DSENet/DN_model/HSENet.keras'\n",
    "model = load_model(\n",
    "    model_save_path,\n",
    "    custom_objects={\n",
    "        'SwinTransformerBlock': SwinTransformerBlock,\n",
    "        'PSNRLoss': PSNRLoss,\n",
    "        'SSIMLoss': SSIMLoss,\n",
    "        'CharbonnierLoss': CharbonnierLoss,\n",
    "        'EdgeLoss': EdgeLoss,\n",
    "        'psnr_metric': psnr_metric,\n",
    "        'ssim_metric': ssim_metric,\n",
    "        'combined_loss': combined_loss\n",
    "    }\n",
    ")\n",
    "\n",
    "# Now you can use your loaded model for inference or further training\n",
    "\n",
    "# Function to load images\n",
    "def load_images(noisy_dir, clean_dir, image_size=(128, 128), max_images=20000):\n",
    "    noisy_images = []\n",
    "    clean_images = []\n",
    "    noisy_pattern = os.path.join(noisy_dir, '*.*')\n",
    "    clean_pattern = os.path.join(clean_dir, '*.*')\n",
    "    noisy_files = sorted(glob.glob(noisy_pattern))[:max_images]\n",
    "    clean_files = sorted(glob.glob(clean_pattern))[:max_images]\n",
    "\n",
    "    for noisy_path, clean_path in zip(noisy_files, clean_files):\n",
    "        noisy_image = cv2.imread(noisy_path)\n",
    "        if noisy_image is None:\n",
    "            print(f\"Warning: Unable to read noisy image file {noisy_path}\")\n",
    "            continue\n",
    "\n",
    "        clean_image = cv2.imread(clean_path)\n",
    "        if clean_image is None:\n",
    "            print(f\"Warning: Unable to read clean image file {clean_path}\")\n",
    "            continue\n",
    "\n",
    "        noisy_image = cv2.resize(noisy_image, image_size)\n",
    "        clean_image = cv2.resize(clean_image, image_size)\n",
    "\n",
    "        noisy_images.append(noisy_image / 255.0)\n",
    "        clean_images.append(clean_image / 255.0)\n",
    "\n",
    "    return np.array(noisy_images), np.array(clean_images)\n",
    "\n",
    "# Generate denoised images using the model\n",
    "start_time = time.time()\n",
    "try:\n",
    "    denoised_images_test = model.predict(X_test)\n",
    "except ValueError as e:\n",
    "    print(f\"Error during prediction: {e}\")\n",
    "    print(\"Resizing test images to match model input size...\")\n",
    "    X_test_resized = np.array([cv2.resize(img, (128, 128)) for img in X_test])\n",
    "    denoised_images_test = model.predict(X_test_resized)\n",
    "end_time = time.time()\n",
    "\n",
    "denosising_time = end_time - start_time\n",
    "print(f\"Time taken for denoising {len(X_test)} patches of size 128x128: {denosising_time:.4f} seconds\")\n",
    "\n",
    "# Create output directories to save images\n",
    "output_dir = '/Final_DN_Traning/DSENet/DN_model/HSENet_v8_merged_Denoised_400_images_same2'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save all images (noisy, clean, and denoised) into the output directory\n",
    "for idx in range(len(X_test)):\n",
    "    noisy_image_path = os.path.join(output_dir, f'noisy_image_{idx}.png')\n",
    "    clean_image_path = os.path.join(output_dir, f'clean_image_{idx}.png')\n",
    "    denoised_image_path = os.path.join(output_dir, f'denoised_image_{idx}.png')\n",
    "\n",
    "    # Convert RGB to BGR before saving to maintain original colors\n",
    "    cv2.imwrite(noisy_image_path, cv2.cvtColor((X_test[idx] * 255).astype(np.uint8), cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(clean_image_path, cv2.cvtColor((y_test[idx] * 255).astype(np.uint8), cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(denoised_image_path, cv2.cvtColor((denoised_images_test[idx] * 255).astype(np.uint8), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# Calculate and display mean PSNR and SSIM for all denoised images\n",
    "psnr_values = []\n",
    "ssim_values = []\n",
    "for idx in range(len(X_test)):\n",
    "    psnr_value = peak_signal_noise_ratio(y_test[idx], denoised_images_test[idx], data_range=1.0)\n",
    "    ssim_value = structural_similarity(y_test[idx], denoised_images_test[idx], channel_axis=-1, data_range=1.0, win_size=7)\n",
    "    psnr_values.append(psnr_value)\n",
    "    ssim_values.append(ssim_value)\n",
    "\n",
    "mean_psnr = np.mean(psnr_values)\n",
    "mean_ssim = np.mean(ssim_values)\n",
    "print(f\"Mean PSNR: {mean_psnr:.4f}\")\n",
    "print(f\"Mean SSIM: {mean_ssim:.4f}\")\n",
    "\n",
    "# Plot random results\n",
    "def plot_random_results(noisy_images, clean_images, denoised_images, n=5):\n",
    "    \"\"\"\n",
    "    Plot random results from noisy, clean, and denoised images.\n",
    "    \n",
    "    Parameters:\n",
    "        noisy_images (numpy array): Array of noisy images.\n",
    "        clean_images (numpy array): Array of clean images.\n",
    "        denoised_images (numpy array): Array of denoised images.\n",
    "        n (int): Number of random images to plot.\n",
    "    \"\"\"\n",
    "    # Select random indices from the test set\n",
    "    indices = random.sample(range(len(noisy_images)), n)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, idx in enumerate(indices):\n",
    "        # Plot Noisy Image\n",
    "        plt.subplot(n, 3, i * 3 + 1)\n",
    "        plt.imshow(noisy_images[idx])\n",
    "        plt.title('Noisy Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plot Clean Image\n",
    "        plt.subplot(n, 3, i * 3 + 2)\n",
    "        plt.imshow(clean_images[idx])\n",
    "        plt.title('Clean Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plot Denoised Image\n",
    "        plt.subplot(n, 3, i * 3 + 3)\n",
    "        plt.imshow(denoised_images[idx])\n",
    "        plt.title('Denoised Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to visualize random results\n",
    "plot_random_results(X_test, y_test, denoised_images_test, n=5)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
